---
layout: default
title: "Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		IEEE Transactions on Robotics (T-RO) 2024<br>
		<br>
		<nobr>Yunho Kim</nobr> &emsp;&emsp; <nobr>Hyunsik Oh</nobr> &emsp;&emsp; <nobr>Jeonghyun Lee</nobr> &emsp;&emsp; <nobr>Jinhyeok Choi</nobr><br>
        <nobr>Gwanghyeon Ji</nobr> &emsp;&emsp; <nobr>Moonkyu Jung</nobr> &emsp;&emsp; <nobr>Donghoon Youm</nobr> &emsp;&emsp; <nobr>Jemin Hwangbo</nobr><br>
		<br>
		<nobr>Korea Advanced Institute of Science and Technology (KAIST)</nobr><br>
		<br>
		<img style="vertical-align:middle" src="teaser.png"  width="100%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	Several earlier studies have shown impressive control performance in complex robotic systems by designing 
    the controller using a neural network and training it with model-free reinforcement learning. However, 
    these outstanding controllers with natural motion style and high task performance are developed through 
    extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous 
    reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement 
    learning framework for training neural network controllers for complex robotic systems consisting of both 
    rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle 
    them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm 
    are suggested. The learning framework is applied to train locomotion controllers for several legged robots with 
    different morphology and physical attributes to traverse challenging terrains. Extensive simulation and real-world 
    experiments demonstrate that performant controllers can be trained with significantly less reward engineering, by 
    tuning only a single reward coefficient. Furthermore, a more straightforward and intuitive engineering process 
    can be utilized, thanks to the interpretability and generalizability of constraints.
</td>

<td>
	<h3> 
		Paper: [<a href="https://ieeexplore.ieee.org/document/10530429">link</a>] &nbsp; &nbsp; &nbsp; 
		ArXiv preprint: [<a href="https://arxiv.org/abs/2308.12517">link</a>] &nbsp; &nbsp; &nbsp;
        Experiment video: [<a href="https://youtu.be/f696z7Mx6Ls?feature=shared">video 1</a>] [<a href="https://youtu.be/GEkC6nJehjo?feature=shared">video 2</a>] [<a href="https://youtu.be/yV-tP3DHXes?feature=shared">video 3</a>] </h3>
</td>

<tr>
		<h3 style="margin-bottom:10px;">Videos</h3>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/KAlm3yskhvM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@ARTICLE{kim2024legConstrainedRL,
    author={Kim, Yunho and Oh, Hyunsik and Lee, Jeonghyun and Choi, Jinhyeok and Ji, Gwanghyeon and Jung, Moonkyu and Youm, Donghoon and Hwangbo, Jemin},
    journal={IEEE Transactions on Robotics}, 
    title={Not Only Rewards but Also Constraints: Applications on Legged Robot Locomotion}, 
    year={2024},
    volume={40},
    number={},
    pages={2984-3003},
    keywords={Robots;Legged locomotion;Reinforcement learning;Optimization;Neural networks;Quadrupedal robots;Training;Constrained reinforcement learning (RL);legged locomotion;RL},
    doi={10.1109/TRO.2024.3400935}
}
</pre>

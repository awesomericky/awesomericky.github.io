---
layout: default
title: "Learning Semantic Traversability with Egocentric Video and Automated Annotation Strategy"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		IEEE Robotics and Automation Letters (RA-L) 2024<br>
		<br>
		<nobr>Yunho Kim* (1, 2)</nobr> &emsp;&emsp; <nobr>Jeong Hyun Lee* (1)</nobr> &emsp;&emsp; <nobr>Choongin Lee (1)</nobr> &emsp;&emsp; <nobr>Juhyeok Mun (1)</nobr><br>
        <nobr>Donghoon Youm (1)</nobr> &emsp;&emsp; <nobr>Jeongsoo Park (1)</nobr> &emsp;&emsp; <nobr>Jemin Hwangbo (1)</nobr><br>
		<br>
		<nobr>(1) Korea Advanced Institute of Science and Technology (KAIST)</nobr> &emsp;&emsp; <nobr>(2) Neuromeka</nobr><br>
		<br>
        <nobr>* indicates equal contribution</nobr><br>
        <br>
		<img style="vertical-align:middle" src="teaser.png"  width="100%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	For reliable autonomous robot navigation in urban settings, the robot must have the ability to identify semantically traversable terrains in the image based on the semantic understanding of the scene. 
    This reasoning ability is based on semantic traversability, which is frequently achieved using semantic segmentation models fine-tuned on the testing domain. 
    This fine-tuning process often involves manual data collection with the target robot and annotation by human labelers which is prohibitively expensive and unscalable. 
    In this work, we present an effective methodology for training a semantic traversability estimator using egocentric videos and an automated annotation process. 
    Egocentric videos are collected from a camera mounted on a pedestrian's chest. 
    The dataset for training the semantic traversability estimator is then automatically generated by extracting semantically traversable regions in each video frame using a recent foundation model in image segmentation and its prompting technique. 
    Extensive experiments with videos taken across several countries and cities, covering diverse urban scenarios, demonstrate the high scalability and generalizability of the proposed annotation method. 
    Furthermore, performance analysis and real-world deployment for autonomous robot navigation showcase that the trained semantic traversability estimator is highly accurate, able to handle diverse camera viewpoints, computationally light, and real-world applicable.
</td>

<td>
	<h3> 
        Paper: [<a href="https://ieeexplore.ieee.org/document/10705064">link</a>] &nbsp; &nbsp; &nbsp; 
		ArXiv preprint: [<a href="https://arxiv.org/abs/2406.02989">link</a>] &nbsp; &nbsp; &nbsp;
</td>

<tr>
		<h3 style="margin-bottom:10px;">Videos</h3>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/EUVoH-wA-lA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</tr>
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@ARTICLE{kim2024egoSemTrav,
    author={Kim, Yunho and Lee, Jeong Hyun and Lee, Choongin and Mun, Juhyeok and Youm, Donghoon and Park, Jeongsoo and Hwangbo, Jemin},
    journal={IEEE Robotics and Automation Letters}, 
    title={Learning Semantic Traversability With Egocentric Video and Automated Annotation Strategy}, 
    year={2024},
    volume={9},
    number={11},
    pages={10423-10430},
    keywords={Semantics;Robots;Cameras;Annotations;Navigation;Training;Semantic segmentation;Data collection;Visualization;Robot vision systems;Deep learning for visual perception;semantic scene understanding;vision-based navigation},
    doi={10.1109/LRA.2024.3474548}
}
</pre>

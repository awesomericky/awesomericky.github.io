---
layout: default
title: Yunho (Ricky) Kim
---

<h1> 
	Yunho (Ricky) Kim
</h1>
<!-- <a href="mailto:xbpeng@berkeley.edu">xbpeng@berkeley.edu</a> -->
<table border="0" cellpadding="3" cellspacing="10">
	<tr>
	<td style="vertical-align:top">
		<img padding="0px 20px 15px 0px" src="images/profile.jpeg"  width="220" height="inherit" border="1px" alt="">
	</td>
	<td>
        I am a roboticist at <a href="https://en.neuromeka.com/">Neuromeka</a> AI team.
        Currently, I am undertaking research on bi-manual manipulation and high-level motion planning, mentored by <a href="https://x.com/junja941">Joonho Lee</a>.

        Before joining Neuromeka, I received my MS at KAIST (Korea Advanced Institute of Science and Technology) and BS at Seoul National University, both majoring in mechanical engineering.
        During my study at KAIST, I was fortunate to be advised by Prof. <a href="https://www.railab.kaist.ac.kr/">Jemin Hwangbo</a> and conducted research on legged robot locomotion and navigation.

		<ul>
			<li>Contact: yunho.kim@neuromeka.com, kimyunho1999@gmail.com</li>
            <li>Google Scholar: <a href="https://scholar.google.co.kr/citations?user=5X2VWqEAAAAJ&hl=en">Link</a></li>
			<li>GitHub: <a href="https://github.com/awesomericky">Link</a></li>
			<li>LinkedIn: <a href="https://www.linkedin.com/in/awesomericky/">Link</a></li>
		</ul>
	</td>
	</tr>
</table>

<hr>

<h2>Publications</h2>
<table border="0" cellpadding="3" cellspacing="10"><tr>
	<tr>
		<td><img style="vertical-align:middle" src="projects/Semantic_traversability/thumb.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
		<td>
			<b>Learning Semantic Traversability with Egocentric Video and Automated Annotation Strategy</b> <br> 
            <b>Yunho Kim*</b>, Jeong Hyun Lee*, Choongin Lee, Juhyeok Mun, Donghoon Youm, Jeongsoo Park, Jemin Hwangbo<br> 
			<em> IEEE Robotics and Automation Letters (RA-L) 2024 </em> <br> 
			[<a href="projects/Semantic_traversability/index.html">Project page</a>] [<a href="https://youtu.be/EUVoH-wA-lA?si=68x_p39mBpScC-QR">Summary video</a>]
		</td>
	</tr>
</table>
<table border="0" cellpadding="3" cellspacing="10"><tr>
	<tr>
		<td><img style="vertical-align:middle" src="projects/Legged_robot_constrained_rl/thumb.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
		<td>
			<b>Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion</b> <br> 
			<b>Yunho Kim</b>, Hyunsik Oh, Jeonghyun Lee, Jinhyeok Choi, Gwanghyeon Ji, Moonkyu Jung, Donghoon Youm, Jemin Hwangbo<br> 
			<em> IEEE Transactions on Robotics (T-RO) 2024 </em> <br> 
			[<a href="projects/Legged_robot_constrained_rl/index.html">Project page</a>] [<a href="https://youtu.be/KAlm3yskhvM?si=FvIdMgmDjBs04MFB">Summary video</a>]
		</td>
	</tr>
</table>
<table border="0" cellpadding="3" cellspacing="10"><tr>
	<tr>
		<td><img style="vertical-align:middle" src="projects/SGPO/SGPO_thumb.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
		<td>
			<b>Safety Guided Policy Optimization</b> <br> 
			Dohyeong Kim, <b>Yunho Kim</b>, Kyungjae Lee, Songhwai Oh<br> 
			<em> International Conference on Intelligent Robots and Systems (IROS) 2022 </em> <br> 
			[<a href="https://ieeexplore.ieee.org/document/9981030">Paper</a>] [<a href="https://www.youtube.com/watch?v=hVmK0bCX_2k">Summary video</a>] 
		</td>
	</tr>
</table>
<table border="0" cellpadding="3" cellspacing="10"><tr>
	<tr>
		<td><img style="vertical-align:middle" src="projects/FDM_ITS_navigation/FDM_ITS_navigation_thumb.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
		<td>
			<b>Learning Forward Dynamics Model and Informed Trajectory Sampler for Safe Quadruped Navigation</b> <br> 
			<b>Yunho Kim</b>, Chanyoung Kim, Jemin Hwangbo<br> 
			<em> Robotics: Science and Systems (RSS) 2022 </em> <br> 
			[<a href="projects/FDM_ITS_navigation/index.html">Project page</a>] [<a href="https://www.youtube.com/watch?v=-UTEL6zHNv4">Summary video</a>]
		</td>
	</tr>
</table>

<h2>Personal Projects</h2>

<table border="0" cellpadding="3" cellspacing="10"><tr>
	<tr>
		<td><img style="vertical-align:middle" src="projects/Perceptive_locomotion/thumb.png"  width="200" height="inherit" border="1px" alt="" /></td>
		<td>
			<b>Perceptive locomotion</b> <br> 
			[<a href="https://www.youtube.com/watch?v=uzO2X95Fsco&t=10s">Point-goal command video</a>] [<a href="https://www.youtube.com/watch?v=rmVL3M7VG6A">Velocity command video</a>] <br> 
			<br> 
			Design perceptive locomotion controllers for quadruped robots using deep reinforcement learning.
		</td>
	</tr>
</table>

<table border="0" cellpadding="3" cellspacing="10"><tr>
	<tr>
		<td><img style="vertical-align:middle" src="projects/Terrain_mapping/thumb.png"  width="200" height="inherit" border="1px" alt="" /></td>
		<td>
			<b>Terrain mapping</b> <br> 
			[<a href="https://youtu.be/BhTm0X-oL7I?feature=shared">Simulation video</a>] [<a href="https://youtu.be/lYDkYvtaQXM?feature=shared">Real-world video</a>] <br> 
			<br> 
			Implement 2.5D terrain mapping pipeline in both the simulation and the real world.
		</td>
	</tr>
</table>

<table border="0" cellpadding="3" cellspacing="10"><tr>
	<tr>
		<td><img style="vertical-align:middle" src="projects/Multiple_gait_locomotion/Multiple_gait_locomotion_thumb.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
		<td>
			<b>Learning Multiple Gaits of Quadruped Robot Using Hierarchical Reinforcement Learning</b> <br> 
			[<a href="projects/Multiple_gait_locomotion/index.html">Project page</a>] <br> 
			<br> 
			Propose a multiple-gait learning framework inspired by central pattern generators.
		</td>
	</tr>
</table>

<table border="0" cellpadding="3" cellspacing="10"><tr>
	<tr>
		<td><img style="vertical-align:middle" src="projects/Speech2Pickup/speech2pickup_thumb.png"  width="200" height="inherit" border="1px" alt="" /></td>
		<td>
			<b>Speech2Pickup: Speech Embedding Based Human-Robot Collaboration Model for Multi Object Robot Grasping Task</b> <br> 
			[<a href="https://github.com/awesomericky/Speech2Pickup">Code</a>] <br> 
			<br> 
			Process data and train a deep neural network to detect objects given speech commands.
		</td>
	</tr>
</table>

<table border="0" cellpadding="3" cellspacing="10"><tr>
	<tr>
		<td><img style="vertical-align:middle" src="projects/RCcar/rccar_thumb.png"  width="200" height="inherit" border="1px" alt="" /></td>
		<td>
			<b>Autonomous RC Car</b> <br> 
			[<a href="https://github.com/awesomericky/autonomous-racecar-project">Code</a>] <br> 
			<br> 
			Implement path tracking and planning algorithms for autonomous RC cars.
		</td>
	</tr>
</table>
